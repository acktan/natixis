{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "f15777ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "# Regressors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from tpot import TPOTRegressor\n",
    "# Classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from tpot import TPOTClassifier\n",
    "# Sentiment analysis\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "tokenizer1 = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b44aa9",
   "metadata": {},
   "source": [
    "# <font color ='red'> Creating data variables </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "35913dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Reading in data #####\n",
    "# Training data with target\n",
    "equity = pd.read_json('data/train/EURUSDV1M_1w.json')\n",
    "volatility = pd.read_json('data/train/VIX_1w.json')\n",
    "# Testing data without target\n",
    "e = pd.read_json('data/test/EURUSDV1M_1w.json')\n",
    "v = pd.read_json('data/test/VIX_1w.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0a9cb",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d13f75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equity\n",
    "stock_e = pd.DataFrame(equity.stock.to_list())\n",
    "target_e_r = pd.DataFrame(equity.target_reg)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(stock_e, target_e_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119535f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatily\n",
    "stock_v = pd.DataFrame(volatility.stock.to_list())\n",
    "target_v_r = pd.DataFrame(volatility.target_reg)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(stock_v, target_v_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572cae15",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59dfe41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equity\n",
    "target_e_c = pd.DataFrame(equity.target_classif)\n",
    "X_train, X_test, y_train, y_test = train_test_split(stock_e, target_e_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0bbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatily\n",
    "target_v_c = pd.DataFrame(volatility.target_classif)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(stock_v, target_v_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a471f5",
   "metadata": {},
   "source": [
    "# <font color = 'red'>Training models without central bank statements</font>\n",
    "Since we have tabular data, we know that trees will perform better than neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37917f1",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73916f02",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "31edb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_predictor(data, model, area, path, name='pred_reg', response='n'):\n",
    "    '''\n",
    "    This functions fits a model to data, makes predictions and offers the possibility to save the predictions\n",
    "    in a .txt file if one is content with the RMSE of the predictions\n",
    "    ----------\n",
    "    Paramters:\n",
    "    data takes a DataFrame with the features to predict from and the target to predict\n",
    "    model is the model used to make the predictions. It must have a fit() and predict() method\n",
    "    area takes the data that should be predicted from after the model has been trained\n",
    "    path is the folder to save the predictions in\n",
    "    name is the name of the txt file to which the predictions can be saved\n",
    "    response decides whether the results should be saved (y) or not (n), the default is not\n",
    "    '''\n",
    "    \n",
    "    # Create the variable with the stock prices\n",
    "    stock = pd.DataFrame(data.stock.to_list())\n",
    "    # Create the variable with the regression target\n",
    "    target = pd.DataFrame(data.target_reg)\n",
    "    # Create the train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(stock, target)\n",
    "    # Instantiate the model\n",
    "    r = model()\n",
    "    # Train the model on the training data\n",
    "    r_model = r.fit(X_train, y_train.values.ravel())\n",
    "    # Print the RMSE of the predicitons on the test split\n",
    "    print('RMSE: ', (mean_squared_error(r_model.predict(X_test), y_test))**0.5)\n",
    "    # Ask whether or not the result of the model's prediction on the evaluation data should be saved or not\n",
    "    # response = input('Do you want to save the result? Y/N \\n')\n",
    "    # If the answer should be saved\n",
    "    if response.lower() == 'y':\n",
    "        # Use the model to make predictions based on the evaluation data and save those predictions in a list\n",
    "        reg = list(r_model.predict(area.stock.to_list()))\n",
    "        # Write the predictions to a txt file in the specified path with the specified name\n",
    "        with open(os.path.join(path, name + '.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(list(map(str, reg))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37719e5d",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d4f46",
   "metadata": {},
   "source": [
    "#### Our baseline: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cd8db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3878019384368221\n",
      "Do you want to save the result? Y/N \n",
      "n\n",
      "RMSE:  0.315535854182696\n",
      "Do you want to save the result? Y/N \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "reg_predictor(equity, LinearRegression, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "reg_predictor(volatility, LinearRegression, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c1ffd",
   "metadata": {},
   "source": [
    "#### Extra Trees regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4313fc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.27804486839393255\n",
      "Do you want to save the result? Y/N \n",
      "n\n",
      "RMSE:  0.3768250477294624\n",
      "Do you want to save the result? Y/N \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "reg_predictor(equity, ExtraTreesRegressor, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "reg_predictor(volatility, ExtraTreesRegressor, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8e743",
   "metadata": {},
   "source": [
    "#### Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5a186d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3050366047300447\n",
      "Do you want to save the result? Y/N \n",
      "n\n",
      "RMSE:  0.35178457151690273\n",
      "Do you want to save the result? Y/N \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "reg_predictor(equity, RandomForestRegressor, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "reg_predictor(volatility, RandomForestRegressor, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143681ea",
   "metadata": {},
   "source": [
    "#### HistGradBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0801999a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.33969377039808407\n",
      "Do you want to save the result? Y/N \n",
      "n\n",
      "RMSE:  0.3749545613301794\n",
      "Do you want to save the result? Y/N \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "reg_predictor(equity, HistGradientBoostingRegressor, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "reg_predictor(volatility, HistGradientBoostingRegressor, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca2a45",
   "metadata": {},
   "source": [
    "#### XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c582b479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3138804695299698\n",
      "Do you want to save the result? Y/N \n",
      "n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3674240116676135\n",
      "Do you want to save the result? Y/N \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "reg_predictor(equity, xgb.XGBRegressor, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "reg_predictor(volatility, xgb.XGBRegressor, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314eb9f",
   "metadata": {},
   "source": [
    "#### LightGBModel regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57227761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3305745083812685\n",
      "Do you want to save the result? Y/N \n",
      "n\n",
      "RMSE:  0.33043460622666937\n",
      "Do you want to save the result? Y/N \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "reg_predictor(equity, lgb.LGBMRegressor, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "reg_predictor(volatility, lgb.LGBMRegressor, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383c93b",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780e3eb",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1669e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif_predictor(data, model, area, path, name='pred_classif', response='No'):\n",
    "    '''\n",
    "    This functions fits a model to data, makes predictions and offers the possibility to save the predictions\n",
    "    in a .txt file if one is content with the accuracy of the predictions\n",
    "    ----------\n",
    "    Paramters:\n",
    "    data takes a DataFrame with the features to predict from and the target to predict\n",
    "    model is the model used to make the predictions. It must have a fit() and predict() method\n",
    "    area takes the data that should be predicted from after the model has been trained\n",
    "    path is the folder to save the predictions in\n",
    "    name is the name of the txt file to which the predictions can be saved\n",
    "    '''\n",
    "    \n",
    "    # Create the variable with the stock prices\n",
    "    stock = pd.DataFrame(data.stock.to_list())\n",
    "    # Create the variable with the regression target\n",
    "    target = pd.DataFrame(data.target_classif)\n",
    "    # Create the train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(stock, target)\n",
    "    # Instantiate the model\n",
    "    c = model()\n",
    "    # Train the model on the training data\n",
    "    c_model = c.fit(X_train, y_train.values.ravel())\n",
    "    # Print the RMSE of the predicitons on the test split\n",
    "    print('Accuracy: ', accuracy_score(c_model.predict(X_test), y_test))\n",
    "    # Ask whether or not the result of the model's prediction on the evaluation data should be saved or not\n",
    "    # response = input('Do you want to save the result? Y/N \\n')\n",
    "    # If the answer should be saved\n",
    "    if response.lower() == 'y':\n",
    "        # Use the model to make predictions based on the evaluation data and save those predictions in a list\n",
    "        classif = list(c_model.predict(area.stock.to_list()))\n",
    "        # Write the predictions to a txt file in the specified path with the specified name\n",
    "        with open(os.path.join(path, name + '.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(list(map(str, classif))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbf1ba",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a5569",
   "metadata": {},
   "source": [
    "#### Our baseline: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "40cb09ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6910828025477707\n",
      "Accuracy:  0.6369426751592356\n"
     ]
    }
   ],
   "source": [
    "classif_predictor(equity, LogisticRegression, e,\n",
    "                  'answer/EURUSDV1M_1w', 'pred_classif')\n",
    "classif_predictor(volatility, LogisticRegression, v, \n",
    "                  'answer/VIX_1w', 'pred_classif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb42856",
   "metadata": {},
   "source": [
    "#### Extra Trees classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2574ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6815286624203821\n",
      "Accuracy:  0.6847133757961783\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "classif_predictor(equity, ExtraTreesClassifier, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "classif_predictor(volatility, ExtraTreesClassifier, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f026de7",
   "metadata": {},
   "source": [
    "#### Random Forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "85f032a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6878980891719745\n",
      "Accuracy:  0.7006369426751592\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "classif_predictor(equity, RandomForestClassifier, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "classif_predictor(volatility, RandomForestClassifier, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52e4c8",
   "metadata": {},
   "source": [
    "#### HistGradBoost classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9a55cfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6592356687898089\n",
      "Accuracy:  0.6815286624203821\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "classif_predictor(equity, HistGradientBoostingClassifier, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "classif_predictor(volatility, HistGradientBoostingClassifier, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3091f42",
   "metadata": {},
   "source": [
    "#### XGBoost classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7c87820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:54:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy:  0.6464968152866242\n",
      "[17:54:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6592356687898089\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "classif_predictor(equity, xgb.XGBClassifier, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "classif_predictor(volatility, xgb.XGBClassifier, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5da09",
   "metadata": {},
   "source": [
    "#### LightGBModel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a1bae6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6337579617834395\n",
      "Accuracy:  0.6242038216560509\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "classif_predictor(equity, lgb.LGBMClassifier, e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "classif_predictor(volatility, lgb.LGBMClassifier, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87484b",
   "metadata": {},
   "source": [
    "# <font color = 'red'>Training models with central bank statements</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59ea5e",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Sentiment analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb89754",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ed495711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speeches(data):\n",
    "    # Filling the empty lists with empty string, to allow easier processing later on\n",
    "    for i in range(len(data['speech'])):\n",
    "        for j in range(len(data['speech'][i])):\n",
    "            if data['speech'][i][j]['ECB'] == []:\n",
    "                data['speech'][i][j]['ECB'] = ['']\n",
    "            if data['speech'][i][j]['FED'] == []:\n",
    "                data['speech'][i][j]['FED'] = ['']\n",
    "                \n",
    "    # Creating a dataframe with speeches only\n",
    "    temp = []\n",
    "    for j in data['speech']:\n",
    "        temp.append(pd.concat([pd.DataFrame(i) for i in j], axis=1))\n",
    "    # Since the index were just 0, we set it to a normal row counter and remove the column with the 0-index\n",
    "    speeches = pd.concat(temp).reset_index().iloc[:, 1:]\n",
    "    return speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "f42dc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\", model=model1, tokenizer=tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f2b29fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    temp = []\n",
    "    max_chunk = 210\n",
    "    delete_from = []\n",
    "    \n",
    "    # Replacing all end of sentence signs with <eos>\n",
    "    text = text.replace('. ', '.<eos>')\n",
    "    text = text.replace('? ', '?<eos>')\n",
    "    text = text.replace('! ', '!<eos>')\n",
    "    # Splitting the data into sentences\n",
    "    sentences = text.split('<eos>')\n",
    "    \n",
    "    current_chunk = 0 \n",
    "    chunks = []\n",
    "    # Looping through the sentences and putting them into chunks as long as the chunk size is less than 210.\n",
    "    # The 210 is a value we got through experimenting, that keeps the number of tokens per chunk (containing)\n",
    "    # normal text below 512, since this is the maximum number of tokens the model can handle at any one time.\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk + 1: \n",
    "            if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                chunks[current_chunk].extend(sentence.split(' '))\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence.split(' '))\n",
    "        else:\n",
    "            chunks.append(sentence.split(' '))\n",
    "    # At the end of some speeches there are useless lists of references that cause the tokenizer to split the\n",
    "    # chunks into more than 512 tokens. Thus we remove the first all all chunks after the first one creating\n",
    "    # more than 512 tokens.\n",
    "    for chunk_id in range(len(chunks)):\n",
    "        if len(tokenizer1(' '.join(chunks[chunk_id]))['input_ids']) <= 512:\n",
    "            chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "        else:\n",
    "            print('Deleting from chunk: ' + str(chunk_id))  # Printing which chunk to delete\n",
    "            delete_from.append(chunk_id)\n",
    "            \n",
    "    if len(delete_from) > 0:\n",
    "        del chunks[delete_from[0]:]  # Deleting all chunks after one produced more than 512 tokens\n",
    "    \n",
    "    temp.append(nlp(chunks))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6fa062c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_file(speeches, path, name):\n",
    "    # Looping through all speeches and analyzing their sentiment\n",
    "    sentiment = []\n",
    "\n",
    "    for j in range(len(speeches)):\n",
    "        start = time.time()\n",
    "        non_empty = []\n",
    "        for speech in speeches.iloc[j]:\n",
    "            if speech != '':  # As long as there was a speech,\n",
    "                non_empty.append(sentiment_analysis(speech))  # analyze its sentiment\n",
    "            else:  # If the speech cell is empty,\n",
    "                non_empty.append(2.5)  # just assign the value 2.5 (not the average, not cannot be confused with one of the other values)\n",
    "        print(f'Row {j}:', time.time() - start)\n",
    "        sentiment.append(non_empty)  # Assign the result of the analyses to sentiment\n",
    "    # Write to disk\n",
    "    pd.DataFrame(sentiment).to_csv(path + '/' + name + '.csv')\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "d5c3ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentiment_file(path):\n",
    "    sentiment = pd.read_csv(path)\n",
    "    sentiment = sentiment.drop(columns='Unnamed: 0', axis=1)\n",
    "    most_common_sentiment(sentiment)  # This function acts directly on the sentiment DataFrame\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "d8cf2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the most common sentiment level as the sentiment for the entire speech\n",
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "def most_common_sentiment(df):\n",
    "    for i in range(df.shape[0]):\n",
    "        #filler = 2.5\n",
    "        for j in range (df.shape[1]):\n",
    "            if  df.iloc[i, j] == '2.5':\n",
    "                # Setting the unknown ratings to the previous known one, basically there saying that\n",
    "                # there are only speeches if updates are necessary. Cold start is 2.5\n",
    "                df.iloc[i, j] = 3\n",
    "            else:\n",
    "                temp = ast.literal_eval(str(df.iloc[i, j]))  # Evaluate the string to be able to treat it as a python data structure\n",
    "                for l in range(len(temp[0])):\n",
    "                    lst = []\n",
    "                    lst.append(temp[0][l]['label'][0])\n",
    "                df.iloc[i, j] = int(most_frequent(lst))\n",
    "                #filler = int(most_frequent(lst))\n",
    "        #df.iloc[i, :] = df.iloc[i, :].fillna(df.iloc[i, :].mean().round(3))  # Replacing the unknown ratings with the average of the known ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0060891",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "637e9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Equity #######\n",
    "## For the training data\n",
    "# Extract the speeches from the data\n",
    "speeches = extract_speeches(equity)\n",
    "# Defining the path and file name to which to store the sentiment analysis\n",
    "path = '/Users/charlesnicholas/Documents/Natixis/SentimentAnalysis'\n",
    "name = 'train_equity_sentiment_analysis'\n",
    "# This creates a sentiment analysis and stores it in the specified path with the specified name\n",
    "#sentiments = create_sentiment_file(speeches, path, name)  # Commented out, since it takes days to run\n",
    "# If the sentiment analysis file has already been created, we can read it from the disk\n",
    "sentiments = read_sentiment_file(path + '/' + name + '.csv')\n",
    "sentiments = sentiments.add_prefix('Speech_')\n",
    "s_equity = equity.join(sentiments).drop(columns='speech')\n",
    "\n",
    "## For the test data\n",
    "# Extract the speeches from the data\n",
    "speeches = extract_speeches(e)\n",
    "# Defining the path and file name to which to store the sentiment analysis\n",
    "path = '/Users/charlesnicholas/Documents/Natixis/SentimentAnalysis'\n",
    "name = 'equity_sentiment_analysis_true_test'\n",
    "# This creates a sentiment analysis and stores it in the specified path with the specified name\n",
    "#sentiments = create_sentiment_file(speeches, path, name)  # Commented out, since it takes hours to run\n",
    "# If the sentiment analysis file has already been created, we can read it from the disk\n",
    "sentiments = read_sentiment_file(path + '/' + name + '.csv')\n",
    "sentiments = sentiments.add_prefix('Speech_')\n",
    "s_e = e.join(sentiments).drop(columns='speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "c429618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Volatility #######\n",
    "## For the training data\n",
    "# Extract the speeches from the data\n",
    "speeches = extract_speeches(volatility)\n",
    "# This creates a sentiment analysis and stores it in the specified path with the specified name\n",
    "path = '/Users/charlesnicholas/Documents/Natixis/SentimentAnalysis'\n",
    "name = 'train_volatility_sentiment_analysis'\n",
    "# This creates a sentiment analysis and stores it in the specified path with the specified name\n",
    "#sentiments = create_sentiment_file(speeches, path, name)  # Commented out, since it takes days to run\n",
    "# If the sentiment analysis file has already been created, we can read it from the disk\n",
    "sentiments = read_sentiment_file(path + '/' + name + '.csv')\n",
    "sentiments = sentiments.add_prefix('Speech_')\n",
    "s_volatility = volatility.join(sentiments).drop(columns='speech')\n",
    "\n",
    "## For the test data\n",
    "# Extract the speeches from the data\n",
    "speeches = extract_speeches(v)\n",
    "# This creates a sentiment analysis and stores it in the specified path with the specified name\n",
    "path = '/Users/charlesnicholas/Documents/Natixis/SentimentAnalysis'\n",
    "name = 'volatility_sentiment_analysis_true_test'\n",
    "# This creates a sentiment analysis and stores it in the specified path with the specified name\n",
    "#sentiments = create_sentiment_file(speeches, path, name)  # Commented out, since it takes hours to run\n",
    "# If the sentiment analysis file has already been created, we can read it from the disk\n",
    "sentiments = read_sentiment_file(path + '/' + name + '.csv')\n",
    "sentiments = sentiments.add_prefix('Speech_')\n",
    "s_v = v.join(sentiments).drop(columns='speech')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c7475",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Model training</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954b961",
   "metadata": {},
   "source": [
    "### <font color = 'blue'>Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5d935",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "bd01a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_reg_predictor(data, model, area, path, name='pred_reg', response='n'):\n",
    "    '''\n",
    "    This functions fits a model to data, makes predictions and offers the possibility to save the predictions\n",
    "    in a .txt file if one is content with the RMSE of the predictions\n",
    "    ----------\n",
    "    Paramters:\n",
    "    data takes a DataFrame with the features to predict from and the target to predict\n",
    "    model is the model used to make the predictions. It must have a fit() and predict() method\n",
    "    area takes the data that should be predicted from after the model has been trained\n",
    "    path is the folder to save the predictions in\n",
    "    name is the name of the txt file to which the predictions can be saved\n",
    "    response decides whether the results should be saved (y) or not (n), the default is not\n",
    "    '''\n",
    "    \n",
    "    # Create the variable with the stock prices\n",
    "    stock = pd.DataFrame(data.stock.to_list()).add_prefix('Day_')\n",
    "    # Create the variable with the speech analyses included\n",
    "    features = data.drop(columns=['stock', 'target_classif', 'target_reg']).join(stock)\n",
    "    features = features.astype(float)\n",
    "    # Create the variable with the regression target\n",
    "    target = pd.DataFrame(data.target_reg)\n",
    "    # Create the train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target)\n",
    "    # Instantiate the model\n",
    "    r = model()\n",
    "    # Train the model on the training data\n",
    "    r_model = r.fit(X_train, y_train.values.ravel())\n",
    "    # Print the RMSE of the predicitons on the test split\n",
    "    print('RMSE: ', (mean_squared_error(r_model.predict(X_test), y_test))**0.5)\n",
    "    # Ask whether or not the result of the model's prediction on the evaluation data should be saved or not\n",
    "    #response = input('Do you want to save the result? Y/N \\n')\n",
    "    # If the answer should be saved\n",
    "    if response.lower() == 'y':\n",
    "        # Use the model to make predictions based on the evaluation data and save those predictions in a list\n",
    "        area_stock = pd.DataFrame(area.stock.to_list()).add_prefix('Day_')\n",
    "        area_features = area.drop(columns=['stock']).join(area_stock)\n",
    "        area_features = area_features.astype(float)\n",
    "        reg = list(r_model.predict(area_features))\n",
    "        # Write the predictions to a txt file in the specified path with the specified name\n",
    "        with open(os.path.join(path, name + '.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(list(map(str, reg))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2835493",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a6bea",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "abc86d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.38228181923859755\n",
      "RMSE:  0.4101866231697137\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, LinearRegression, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, LinearRegression, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c84028",
   "metadata": {},
   "source": [
    "#### Extra Tress regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "cb34fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.275192412660448\n",
      "RMSE:  0.40707032756399897\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, ExtraTreesRegressor, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, ExtraTreesRegressor, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02297167",
   "metadata": {},
   "source": [
    "#### Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "fe702150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.298707457793211\n",
      "RMSE:  0.3431408115283878\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, RandomForestRegressor, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, RandomForestRegressor, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f2847",
   "metadata": {},
   "source": [
    "#### HistGradBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "22e6ce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3297811314049579\n",
      "RMSE:  0.3753017486714521\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, HistGradientBoostingRegressor, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, HistGradientBoostingRegressor, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a660e",
   "metadata": {},
   "source": [
    "#### XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "ab4ebda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3441181861640191\n",
      "RMSE:  0.3508529388924772\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, xgb.XGBRegressor, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, xgb.XGBRegressor, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c58b38",
   "metadata": {},
   "source": [
    "#### LightGBModel regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "e1a1033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.36954371472767017\n",
      "RMSE:  0.3833764604874184\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, lgb.LGBMRegressor, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, lgb.LGBMRegressor, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3572a7fd",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b934e",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "09827b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_classif_predictor(data, model, area, path, name='pred_classif', response='n'):\n",
    "    '''\n",
    "    This functions fits a model to data, makes predictions and offers the possibility to save the predictions\n",
    "    in a .txt file if one is content with the Accuracy of the predictions\n",
    "    ----------\n",
    "    Paramters:\n",
    "    data takes a DataFrame with the features to predict from and the target to predict\n",
    "    model is the model used to make the predictions. It must have a fit() and predict() method\n",
    "    area takes the data that should be predicted from after the model has been trained\n",
    "    path is the folder to save the predictions in\n",
    "    name is the name of the txt file to which the predictions can be saved\n",
    "    response decides whether the results should be saved (y) or not (n), the default is not\n",
    "    '''\n",
    "    \n",
    "    # Create the variable with the stock prices\n",
    "    stock = pd.DataFrame(data.stock.to_list()).add_prefix('Day_')\n",
    "    # Create the variable with the speech analyses included\n",
    "    features = data.drop(columns=['stock', 'target_classif', 'target_reg']).join(stock)\n",
    "    features = features.astype(float)\n",
    "    # Create the variable with the regression target\n",
    "    target = pd.DataFrame(data.target_classif)\n",
    "    # Create the train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target)\n",
    "    # Instantiate the model\n",
    "    if 'solver' in model().get_params():\n",
    "        c = model(solver='liblinear')  # This gives us the best result for linear regression\n",
    "    else:\n",
    "        c = model()\n",
    "    # Train the model on the training data\n",
    "    c_model = c.fit(X_train, y_train.values.ravel())\n",
    "    # Print the RMSE of the predicitons on the test split\n",
    "    print('Accuracy: ', accuracy_score(c_model.predict(X_test), y_test))\n",
    "    # Ask whether or not the result of the model's prediction on the evaluation data should be saved or not\n",
    "    #response = input('Do you want to save the result? Y/N \\n')\n",
    "    # If the answer should be saved\n",
    "    if response.lower() == 'y':\n",
    "        # Use the model to make predictions based on the evaluation data and save those predictions in a list\n",
    "        area_stock = pd.DataFrame(area.stock.to_list()).add_prefix('Day_')\n",
    "        area_features = area.drop(columns=['stock']).join(area_stock)\n",
    "        area_features = area_features.astype(float)\n",
    "        classif = list(c_model.predict(area_features))\n",
    "        # Write the predictions to a txt file in the specified path with the specified name\n",
    "        with open(os.path.join(path, name + '.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(list(map(str, classif))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eeae7c",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7af5fe",
   "metadata": {},
   "source": [
    "#### Logistic regression (best for leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "216bce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6656050955414012\n",
      "Accuracy:  0.5955414012738853\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_classif_predictor(s_equity, LogisticRegression, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_classif_predictor(s_volatility, LogisticRegression, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d60c9",
   "metadata": {},
   "source": [
    "#### Extra Trees classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "21a2e5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.697452229299363\n",
      "Accuracy:  0.6656050955414012\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_classif_predictor(s_equity, ExtraTreesClassifier, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_classif_predictor(s_volatility, ExtraTreesClassifier, v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb89101",
   "metadata": {},
   "source": [
    "#### Random Forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "0b2994e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6910828025477707\n",
      "Accuracy:  0.7165605095541401\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_classif_predictor(s_equity, RandomForestClassifier, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_classif_predictor(s_volatility, RandomForestClassifier, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e8c18",
   "metadata": {},
   "source": [
    "#### HistGradBoost classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "755acc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6528662420382165\n",
      "Accuracy:  0.7133757961783439\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_classif_predictor(s_equity, HistGradientBoostingClassifier, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_classif_predictor(s_volatility, HistGradientBoostingClassifier, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce09c1e",
   "metadata": {},
   "source": [
    "#### XGBoost classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "dbc33e1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:30:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy:  0.6751592356687898\n",
      "[04:30:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy:  0.6751592356687898\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_classif_predictor(s_equity, xgb.XGBClassifier, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_classif_predictor(s_volatility, xgb.XGBClassifier, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c333cb",
   "metadata": {},
   "source": [
    "#### LightGBModel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "97408b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6878980891719745\n",
      "Accuracy:  0.6815286624203821\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_classif_predictor(s_equity, lgb.LGBMClassifier, s_e, 'answer/EURUSDV1M_1w')\n",
    "# Volatility\n",
    "s_classif_predictor(s_volatility, lgb.LGBMClassifier, s_v, 'answer/VIX_1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef1b02",
   "metadata": {},
   "source": [
    "# <font color = 'red'>Submissions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20426ba",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75170275",
   "metadata": {},
   "source": [
    "## Using the training set with train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e70627",
   "metadata": {},
   "source": [
    "#### Extra Trees regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "f91df03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.27040698623802467\n",
      "RMSE:  0.39239190744334584\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, ExtraTreesRegressor, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, ExtraTreesRegressor, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf45cb",
   "metadata": {},
   "source": [
    "#### HistGradBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "a0a7eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3097169805320025\n",
      "RMSE:  0.38427795675241827\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, HistGradientBoostingRegressor, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, HistGradientBoostingRegressor, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb956ea",
   "metadata": {},
   "source": [
    "#### LightGBModel regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "fb0f2197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.30504865016237187\n",
      "RMSE:  0.3742112881704846\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, lgb.LGBMRegressor, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, lgb.LGBMRegressor, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62ed3c",
   "metadata": {},
   "source": [
    "#### XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "11d860ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3676016769676427\n",
      "RMSE:  0.3472002394613814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesnicholas/miniforge3/envs/Natixis/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, xgb.XGBRegressor, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, xgb.XGBRegressor, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530a981",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "ad2310a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.3239155089816116\n",
      "RMSE:  0.3952095009206132\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_reg_predictor(s_equity, LinearRegression, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_reg_predictor(s_volatility, LinearRegression, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad316fe0",
   "metadata": {},
   "source": [
    "#### Just taking the last element of the stock price series (best for leader board :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "93116a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equity\n",
    "last_entry = list(e.stock.apply(lambda x: x[-1]))\n",
    "with open('/Users/charlesnicholas/Documents/Natixis/starting_kit_final/answer/EURUSDV1M_1w/pred_reg.txt', 'w') as f:\n",
    "    f.write('\\n'.join(list(map(str, last_entry))))\n",
    "# Volatility\n",
    "last_entry = list(v.stock.apply(lambda x: x[-1]))\n",
    "with open('/Users/charlesnicholas/Documents/Natixis/starting_kit_final/answer/VIX_1w/pred_reg.txt', 'w') as f:\n",
    "    f.write('\\n'.join(list(map(str, last_entry))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df920f6",
   "metadata": {},
   "source": [
    "## Using the entire training set to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9dd48",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "36fb69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_reg_predictor_all(data, model, area, path, name='pred_reg', response='n'):\n",
    "    '''\n",
    "    This functions fits a model to data, makes predictions and offers the possibility to save the predictions\n",
    "    in a .txt file if one is content with the RMSE of the predictions\n",
    "    ----------\n",
    "    Paramters:\n",
    "    data takes a DataFrame with the features to predict from and the target to predict\n",
    "    model is the model used to make the predictions. It must have a fit() and predict() method\n",
    "    area takes the data that should be predicted from after the model has been trained\n",
    "    path is the folder to save the predictions in\n",
    "    name is the name of the txt file to which the predictions can be saved\n",
    "    response decides whether the results should be saved (y) or not (n), the default is not\n",
    "    '''\n",
    "    \n",
    "    # Create the variable with the stock prices\n",
    "    stock = pd.DataFrame(data.stock.to_list()).add_prefix('Day_')\n",
    "    # Create the variable with the speech analyses included\n",
    "    features = data.drop(columns=['stock', 'target_classif', 'target_reg']).join(stock)\n",
    "    features = features.astype(float)\n",
    "    # Create the variable with the regression target\n",
    "    target = pd.DataFrame(data.target_reg)\n",
    "    # Create the train and test set\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(features, target)\n",
    "    # Instantiate the model\n",
    "    r = model()\n",
    "    # Train the model on the training data\n",
    "    r_model = r.fit(features, target.values.ravel())\n",
    "    # Print the RMSE of the predicitons on the test split\n",
    "    #print('RMSE: ', (mean_squared_error(r_model.predict(X_test), y_test))**0.5)\n",
    "    # Ask whether or not the result of the model's prediction on the evaluation data should be saved or not\n",
    "    #response = input('Do you want to save the result? Y/N \\n')\n",
    "    # If the answer should be saved\n",
    "    if response.lower() == 'y':\n",
    "        # Use the model to make predictions based on the evaluation data and save those predictions in a list\n",
    "        area_stock = pd.DataFrame(area.stock.to_list()).add_prefix('Day_')\n",
    "        area_features = area.drop(columns=['stock']).join(area_stock)\n",
    "        area_features = area_features.astype(float)\n",
    "        reg = list(r_model.predict(area_features))\n",
    "        # Write the predictions to a txt file in the specified path with the specified name\n",
    "        with open(os.path.join(path, name + '.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(list(map(str, reg))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46af9b2",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a00e1",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "94085ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equity\n",
    "s_reg_predictor_all(s_equity, LinearRegression, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_reg_predictor_all(s_volatility, LinearRegression, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f3a5f",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8fe4b",
   "metadata": {},
   "source": [
    "## Using the training set with train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811597ba",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "7f8af074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6719745222929936\n",
      "Accuracy:  0.6050955414012739\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_classif_predictor(s_equity, LogisticRegression, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_classif_predictor(s_volatility, LogisticRegression, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d89c2a",
   "metadata": {},
   "source": [
    "#### Extra Trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "58c96a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6592356687898089\n",
      "Accuracy:  0.6528662420382165\n"
     ]
    }
   ],
   "source": [
    "# Equity\n",
    "s_classif_predictor(s_equity, ExtraTreesClassifier, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_classif_predictor(s_volatility, ExtraTreesClassifier, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676012a",
   "metadata": {},
   "source": [
    "## Using the entire training set to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906712c4",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "817d95ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_classif_predictor_all(data, model, area, path, name='pred_classif', response='n'):\n",
    "    '''\n",
    "    This functions fits a model to data, makes predictions and offers the possibility to save the predictions\n",
    "    in a .txt file if one is content with the Accuracy of the predictions\n",
    "    ----------\n",
    "    Paramters:\n",
    "    data takes a DataFrame with the features to predict from and the target to predict\n",
    "    model is the model used to make the predictions. It must have a fit() and predict() method\n",
    "    area takes the data that should be predicted from after the model has been trained\n",
    "    path is the folder to save the predictions in\n",
    "    name is the name of the txt file to which the predictions can be saved\n",
    "    response decides whether the results should be saved (y) or not (n), the default is not\n",
    "    '''\n",
    "    \n",
    "    # Create the variable with the stock prices\n",
    "    stock = pd.DataFrame(data.stock.to_list()).add_prefix('Day_')\n",
    "    # Create the variable with the speech analyses included\n",
    "    features = data.drop(columns=['stock', 'target_classif', 'target_reg']).join(stock)\n",
    "    features = features.astype(float)\n",
    "    # Create the variable with the regression target\n",
    "    target = pd.DataFrame(data.target_classif)\n",
    "    # Create the train and test set\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(features, target)\n",
    "    # Instantiate the model\n",
    "    if 'solver' in model().get_params():\n",
    "        c = model(solver='liblinear')  # This gives us the best result for linear regression\n",
    "    else:\n",
    "        c = model()\n",
    "    # Train the model on the training data\n",
    "    c_model = c.fit(features, target.values.ravel())\n",
    "    # Print the RMSE of the predicitons on the test split\n",
    "    #print('Accuracy: ', accuracy_score(c_model.predict(X_test), y_test))\n",
    "    # Ask whether or not the result of the model's prediction on the evaluation data should be saved or not\n",
    "    #response = input('Do you want to save the result? Y/N \\n')\n",
    "    # If the answer should be saved\n",
    "    if response.lower() == 'y':\n",
    "        # Use the model to make predictions based on the evaluation data and save those predictions in a list\n",
    "        area_stock = pd.DataFrame(area.stock.to_list()).add_prefix('Day_')\n",
    "        area_features = area.drop(columns=['stock']).join(area_stock)\n",
    "        area_features = area_features.astype(float)\n",
    "        classif = list(c_model.predict(area_features))\n",
    "        # Write the predictions to a txt file in the specified path with the specified name\n",
    "        with open(os.path.join(path, name + '.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(list(map(str, classif))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c89902",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f520772",
   "metadata": {},
   "source": [
    "#### Random Forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "3c4fcb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equity\n",
    "s_classif_predictor_all(s_equity, RandomForestClassifier, s_e, 'answer/EURUSDV1M_1w', response='y')\n",
    "# Volatility\n",
    "s_classif_predictor_all(s_volatility, RandomForestClassifier, s_v, 'answer/VIX_1w', response='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3063f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the entire dataset to train produces worse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4b674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Natixis-kernel",
   "language": "python",
   "name": "natixis-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
