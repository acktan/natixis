{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport json\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom textblob import TextBlob","metadata":{"id":"4JHNcN6du5vL","execution":{"iopub.status.busy":"2022-03-06T20:20:35.397742Z","iopub.execute_input":"2022-03-06T20:20:35.398105Z","iopub.status.idle":"2022-03-06T20:20:43.133546Z","shell.execute_reply.started":"2022-03-06T20:20:35.398014Z","shell.execute_reply":"2022-03-06T20:20:43.132708Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_eur = pd.read_json('../input/train-natixis/EURUSDV1M_1w.json')\ntrain_vix = pd.read_json('../input/train-natixis/VIX_1w.json')","metadata":{"id":"REo7-X_i0aq9","outputId":"d018d82e-8f1f-479d-d07b-51b51e5cba31","execution":{"iopub.status.busy":"2022-03-06T20:20:46.405783Z","iopub.execute_input":"2022-03-06T20:20:46.406510Z","iopub.status.idle":"2022-03-06T20:20:53.396062Z","shell.execute_reply.started":"2022-03-06T20:20:46.406472Z","shell.execute_reply":"2022-03-06T20:20:53.395077Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def split_df(df):\n  stock = pd.DataFrame(df['stock'].to_list(), index=df.index)\n  \n  df1 = df.drop(labels='stock', axis=1)\n  df1 = pd.concat([stock, df1], axis=1)\n\n  speeches = pd.DataFrame(df.iloc[:, 0].tolist(), index=df.index)\n  appended_data = []\n  for i in range(0, 20):\n    x = pd.DataFrame(speeches.iloc[:, i].tolist(), index=speeches.index)\n    appended_data.append(x)\n\n  appended_data = pd.concat(appended_data, axis=1)\n  df1 = df1.drop(labels='speech', axis=1)\n  final = pd.concat([appended_data, df1], axis=1)\n  return final\n","metadata":{"id":"Nwy2VuSJ_L9K","execution":{"iopub.status.busy":"2022-03-06T20:20:53.397398Z","iopub.execute_input":"2022-03-06T20:20:53.397604Z","iopub.status.idle":"2022-03-06T20:20:53.404165Z","shell.execute_reply.started":"2022-03-06T20:20:53.397579Z","shell.execute_reply":"2022-03-06T20:20:53.403623Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nsummarizer = pipeline('summarization')","metadata":{"id":"OvyE8BLoe1YM","outputId":"221a27eb-6cad-44fb-9963-1d4d04524519","execution":{"iopub.status.busy":"2022-03-06T20:20:53.405202Z","iopub.execute_input":"2022-03-06T20:20:53.405550Z","iopub.status.idle":"2022-03-06T20:22:38.286374Z","shell.execute_reply.started":"2022-03-06T20:20:53.405512Z","shell.execute_reply":"2022-03-06T20:22:38.285657Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#attempt 1: textblob\nsplit_train_eur = split_df(train_eur)\nsplit_train_vix = split_df(train_vix)\n\nfor i in tqdm(range(split_train_eur.shape[0])):\n    for j in range(40):\n        split_train_eur.iloc[i,j] = TextBlob(str(split_train_eur.iloc[i,j])).sentiment\n        split_train_eur.iloc[i,j] = split_train_eur.iloc[i,j][0]","metadata":{"execution":{"iopub.status.busy":"2022-03-06T20:22:38.287712Z","iopub.execute_input":"2022-03-06T20:22:38.287910Z","iopub.status.idle":"2022-03-06T20:27:08.173208Z","shell.execute_reply.started":"2022-03-06T20:22:38.287887Z","shell.execute_reply":"2022-03-06T20:27:08.172179Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\n\nX_train, X_test, y_train, y_test = train_test_split(split_train_eur.iloc[:, 0:60], \n                                                    split_train_eur.iloc[:, 60],\n                                                    test_size=0.3)\n\nsvm = SVC()\nknn = KNeighborsClassifier(n_neighbors=7)\nlgbm = lgb.LGBMClassifier()\nxgbm = xgb.XGBClassifier()\nrtf = RandomForestClassifier()\netc = ExtraTreesClassifier()\n\nsvm_model = svm.fit(X_train, y_train)\nknn_model = knn.fit(X_train, y_train)\netc_model = etc.fit(X_train, y_train)\nrtf_model = rtf.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T18:56:45.956054Z","iopub.execute_input":"2022-03-06T18:56:45.956655Z","iopub.status.idle":"2022-03-06T18:56:46.560381Z","shell.execute_reply.started":"2022-03-06T18:56:45.956614Z","shell.execute_reply":"2022-03-06T18:56:46.559679Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('Accuracy score (SVM): ', accuracy_score(svm_model.predict(X_test), y_test))\nprint('Accuracy score (KNN): ', accuracy_score(knn_model.predict(X_test), y_test))\nprint('Accuracy score (ETC): ', accuracy_score(etc_model.predict(X_test), y_test))\nprint('Accuracy score (RTF): ', accuracy_score(rtf_model.predict(X_test), y_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-06T18:56:47.759796Z","iopub.execute_input":"2022-03-06T18:56:47.760055Z","iopub.status.idle":"2022-03-06T18:56:47.909281Z","shell.execute_reply.started":"2022-03-06T18:56:47.760025Z","shell.execute_reply":"2022-03-06T18:56:47.908495Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(split_train_eur.iloc[:, 0:60], \n                                                    split_train_eur.iloc[:, 61],\n                                                    test_size=0.3)\n\nrtf = RandomForestRegressor(random_state=42)\netc = ExtraTreesRegressor(random_state=42)\nlin = LinearRegression()\n\netc_model = etc.fit(X_train, y_train)\nrtf_model = rtf.fit(X_train, y_train)\nlin_model = lin.fit(X_train, y_train)\n\nprint('Accuracy score (ETC): ', (mean_squared_error(etc_model.predict(X_test), y_test))**0.5)\nprint('Accuracy score (RTF): ', (mean_squared_error(rtf_model.predict(X_test), y_test))**0.5)\nprint('Accuracy score (LIN): ', (mean_squared_error(lin_model.predict(X_test), y_test))**0.5)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:02:48.350553Z","iopub.execute_input":"2022-03-06T19:02:48.350799Z","iopub.status.idle":"2022-03-06T19:02:50.617843Z","shell.execute_reply.started":"2022-03-06T19:02:48.350770Z","shell.execute_reply":"2022-03-06T19:02:50.615421Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#attempt 2: FinBERT\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \n             \"growth is strong and we have plenty of liquidity\", \n             \"there are doubts about our finances\", \n             \"profits are flat\"]\nresults = nlp(sentences)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:14:36.556354Z","iopub.execute_input":"2022-03-06T19:14:36.557069Z","iopub.status.idle":"2022-03-06T19:14:53.918367Z","shell.execute_reply.started":"2022-03-06T19:14:36.557032Z","shell.execute_reply":"2022-03-06T19:14:53.916921Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"split_train_eur = split_df(train_eur)\nsplit_train_vix = split_df(train_vix)\n\nmax_chunk = 250\ncell = []\nfor i in tqdm(range(split_train_eur.shape[0])):\n    for j in range(split_train_eur.shape[1]):\n        if split_train_eur.iloc[i,j] != []:\n            cell = split_train_eur.iloc[i,j][0]\n            cell = cell.replace('.', '.<eos>')\n            cell = cell.replace('?', '?<eos>')\n            cell = cell.replace('!', '!<eos>')\n            sentences = cell.split('<eos>')\n            current_chunk = 0 \n            chunks = []\n            for sentence in sentences:\n                if len(chunks) == current_chunk + 1: \n                    if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n                        chunks[current_chunk].extend(sentence.split(' '))\n                    else:\n                        current_chunk += 1\n                        chunks.append(sentence.split(' '))\n                else:\n                    chunks.append(sentence.split(' '))\n\n            for chunk_id in range(len(chunks)):\n                chunks[chunk_id] = ' '.join(chunks[chunk_id])\n            res = nlp(chunks)\n            split_train_eur.iloc[i,j] = ' '.join([summ['label'] for summ in res])\n        else:\n            None\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T20:05:09.864606Z","iopub.execute_input":"2022-03-06T20:05:09.865145Z","iopub.status.idle":"2022-03-06T20:10:24.928209Z","shell.execute_reply.started":"2022-03-06T20:05:09.865106Z","shell.execute_reply":"2022-03-06T20:10:24.927138Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"#attempt 3: bert base multilingual\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n\nnlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T20:27:46.313476Z","iopub.execute_input":"2022-03-06T20:27:46.313822Z","iopub.status.idle":"2022-03-06T20:28:49.941079Z","shell.execute_reply.started":"2022-03-06T20:27:46.313791Z","shell.execute_reply":"2022-03-06T20:28:49.940161Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"split_train_eur = split_df(train_eur)\nsplit_train_vix = split_df(train_vix)\n\n\nmax_chunk = 250\ncell = []\nfor i in tqdm(range(split_train_eur.shape[0])):\n    for j in range(split_train_eur.shape[1]):\n        if split_train_eur.iloc[i,j] != []:\n            cell = split_train_eur.iloc[i,j][0]\n            cell = cell.replace('.', '.<eos>')\n            cell = cell.replace('?', '?<eos>')\n            cell = cell.replace('!', '!<eos>')\n            sentences = cell.split('<eos>')\n            current_chunk = 0 \n            chunks = []\n            for sentence in sentences:\n                if len(chunks) == current_chunk + 1: \n                    if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n                        chunks[current_chunk].extend(sentence.split(' '))\n                    else:\n                        current_chunk += 1\n                        chunks.append(sentence.split(' '))\n                else:\n                    chunks.append(sentence.split(' '))\n\n            for chunk_id in range(len(chunks)):\n                chunks[chunk_id] = ' '.join(chunks[chunk_id])\n            res = nlp(chunks)\n            split_train_eur.iloc[i,j] = ' '.join([summ['label'] for summ in res])\n        else:\n            None\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T20:29:43.078936Z","iopub.execute_input":"2022-03-06T20:29:43.079318Z","iopub.status.idle":"2022-03-06T23:17:37.491130Z","shell.execute_reply.started":"2022-03-06T20:29:43.079281Z","shell.execute_reply":"2022-03-06T23:17:37.489096Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}